@inproceedings{quExploitingLinearModels2021,
 abstract = {Model-free learning-based control methods have seen great success recently. However, such methods typically suffer from poor sample complexity and limited convergence guarantees. This is in sharp contrast to classical model-based control, which has a rich theory but typically requires strong modeling assumptions. In this paper, we combine the two approaches. We consider a dynamical system with both linear and non-linear components and use the linear model to define a warm start for a model-free, policy gradient method. We show this hybrid approach outperforms the model-based controller while avoiding the convergence issues associated with model-free approaches via both numerical experiments and theoretical analyses, in which we derive sufficient conditions on the non-linear component such that our approach is guaranteed to converge to the (nearly) global optimal controller.},
 author = {Qu, Guannan and Yu, Chenkai and Low, Steven and Wierman, Adam},
 booktitle = {2021 60th IEEE Conference on Decision and Control (CDC)},
 doi = {10.1109/CDC45484.2021.9683735},
 issn = {2576-2370},
 keywords = {Analytical models,Neural networks,Numerical models,Reliability theory,Search methods,Solid modeling,Sufficient conditions},
 month = {December},
 pages = {6539--6546},
 shorttitle = {Exploiting Linear Models for Model-Free Nonlinear Control},
 title = {Exploiting Linear Models for Model-Free Nonlinear Control: A Provably Convergent Policy Gradient Approach},
 urldate = {2025-04-16},
 year = {2021}
}
